{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим требуемые нам библиотеки: \n",
    "* `byaldi` - высокоуровневый [фреймворк](https://github.com/AnswerDotAI/byaldi) непосредственно для работы с моделями ColPali\n",
    "* `pdf2image` - для перевода `.pdf`-файлов в изображения\n",
    "* `poppler-utils` - для работы `pdf2image`\n",
    "* `Spire.Doc` - для перевода `.docx`-файлов в формат `.pdf`\n",
    "* `qwen-vl-utils` - для работы Qwen-VL-моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-07T08:16:53.946652Z",
     "iopub.status.busy": "2024-12-07T08:16:53.946329Z",
     "iopub.status.idle": "2024-12-07T08:18:09.153939Z",
     "shell.execute_reply": "2024-12-07T08:18:09.152598Z",
     "shell.execute_reply.started": "2024-12-07T08:16:53.946620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libpoppler118 poppler-data\n",
      "Suggested packages:\n",
      "  ghostscript fonts-japanese-mincho | fonts-ipafont-mincho\n",
      "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
      "  fonts-arphic-uming fonts-nanum\n",
      "The following NEW packages will be installed:\n",
      "  libpoppler118 poppler-data poppler-utils\n",
      "0 upgraded, 3 newly installed, 0 to remove and 72 not upgraded.\n",
      "Need to get 3427 kB of archives.\n",
      "After this operation, 17.7 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2171 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.5 [1071 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
      "Fetched 3427 kB in 0s (23.5 MB/s)\n",
      "Selecting previously unselected package poppler-data.\n",
      "(Reading database ... 122997 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-data_0.4.11-1_all.deb ...\n",
      "Unpacking poppler-data (0.4.11-1) ...\n",
      "Selecting previously unselected package libpoppler118:amd64.\n",
      "Preparing to unpack .../libpoppler118_22.02.0-2ubuntu0.5_amd64.deb ...\n",
      "Unpacking libpoppler118:amd64 (22.02.0-2ubuntu0.5) ...\n",
      "Selecting previously unselected package poppler-utils.\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
      "Setting up poppler-data (0.4.11-1) ...\n",
      "Setting up libpoppler118:amd64 (22.02.0-2ubuntu0.5) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "tesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
      "tesseract-ocr-eng set to manually installed.\n",
      "The following NEW packages will be installed:\n",
      "  tesseract-ocr-rus\n",
      "0 upgraded, 1 newly installed, 0 to remove and 72 not upgraded.\n",
      "Need to get 1271 kB of archives.\n",
      "After this operation, 3877 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-rus all 1:4.00~git30-7274cfa-1.1 [1271 kB]\n",
      "Fetched 1271 kB in 1s (1785 kB/s)\n",
      "Selecting previously unselected package tesseract-ocr-rus.\n",
      "(Reading database ... 123567 files and directories currently installed.)\n",
      "Preparing to unpack .../tesseract-ocr-rus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
      "Unpacking tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
      "Setting up tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade byaldi -q\n",
    "!sudo apt-get install -y poppler-utils -q\n",
    "!pip install -q pdf2image flash-attn -q\n",
    "!pip install Spire.Doc -q\n",
    "!pip install qwen-vl-utils -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим папку, где у нас лежат первоначальные данные `input_folder`, а так же папку, куда мы переведём файлы непосредственно для работы с RAG-системой `working_folder`. Так же обозначим папку, где у нас лежит готовая RAG-база `rag_input_dir`, и папку, куда мы положим её для поиска релевантных документов `rag_target_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:32:56.020090Z",
     "iopub.status.busy": "2024-12-07T08:32:56.019396Z",
     "iopub.status.idle": "2024-12-07T08:32:56.026585Z",
     "shell.execute_reply": "2024-12-07T08:32:56.025534Z",
     "shell.execute_reply.started": "2024-12-07T08:32:56.020050Z"
    }
   },
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import shutil\n",
    "from spire.doc import *\n",
    "from spire.doc.common import *\n",
    "import gzip\n",
    "import json\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "input_folder = '/kaggle/input/nornikel-2024/'\n",
    "working_folder = '/kaggle/working/nornikel-2024/'\n",
    "rag_input_dir = '/kaggle/input/nornikel-2024-rag'\n",
    "rag_target_dir = '/kaggle/working/nornikel-2024-rag'\n",
    "\n",
    "if not os.path.exists(working_folder):\n",
    "    os.makedirs(working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся работа велась в Kaggle, а при подгрузке данных он автоматически распаковывает все архивы (`.zip`, `.tar.gz`, `.gz`), для загрузки конфигурационных файлов byaldi требует наличия именно заархифированных конфиг-файлов (то есть не `.json`, а `.json.gz`), поэтому перекинем все эмбеддинги документов в рабочий каталог и назад зашифруем конфиги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:18:29.737948Z",
     "iopub.status.busy": "2024-12-07T08:18:29.737406Z",
     "iopub.status.idle": "2024-12-07T08:18:35.485522Z",
     "shell.execute_reply": "2024-12-07T08:18:35.484509Z",
     "shell.execute_reply.started": "2024-12-07T08:18:29.737917Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_files(source, target):\n",
    "    os.makedirs(target, exist_ok=True)\n",
    "\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        rel_path = os.path.relpath(root, source)\n",
    "        target_root = os.path.join(target, rel_path)\n",
    "\n",
    "        os.makedirs(target_root, exist_ok=True)\n",
    "\n",
    "        for filename in files:\n",
    "            source_path = os.path.join(root, filename)\n",
    "            target_path = os.path.join(target_root, filename)\n",
    "\n",
    "            if filename.endswith('.json'):\n",
    "                target_path = target_path.replace('.json', '.json.gz')\n",
    "                with open(source_path, 'rb') as f_in:\n",
    "                    with gzip.open(target_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "            else:\n",
    "                shutil.copy2(source_path, target_path)\n",
    "\n",
    "process_files(rag_input_dir, rag_target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем словарь со всеми нашими документами, чтобы в дальнейшем было проще с ними работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:18:35.487682Z",
     "iopub.status.busy": "2024-12-07T08:18:35.487357Z",
     "iopub.status.idle": "2024-12-07T08:18:35.493327Z",
     "shell.execute_reply": "2024-12-07T08:18:35.492279Z",
     "shell.execute_reply.started": "2024-12-07T08:18:35.487654Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/kaggle/working/nornikel-2024-rag/nornikel_index/doc_ids_to_file_names.json.gz'\n",
    "docs_names = dict()\n",
    "\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "    docs_names = json.load(f)\n",
    "    docs_names = {int(key): value for key, value in docs_names.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:18:35.495216Z",
     "iopub.status.busy": "2024-12-07T08:18:35.494783Z",
     "iopub.status.idle": "2024-12-07T08:18:35.848306Z",
     "shell.execute_reply": "2024-12-07T08:18:35.847337Z",
     "shell.execute_reply.started": "2024-12-07T08:18:35.495167Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_doc_to_pdf(doc_path, pdf_path):\n",
    "    document = Document()\n",
    "    document.LoadFromFile(doc_path)\n",
    "    document.SaveToFile(pdf_path, FileFormat.PDF)\n",
    "    document.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведём все наши `.pdf`-файлы в рабочий каталог, а файлы `.docx` переведём в требуемый для ColPali формат `.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:18:35.849984Z",
     "iopub.status.busy": "2024-12-07T08:18:35.849655Z",
     "iopub.status.idle": "2024-12-07T08:18:42.373296Z",
     "shell.execute_reply": "2024-12-07T08:18:42.372333Z",
     "shell.execute_reply.started": "2024-12-07T08:18:35.849954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Копирован файл: Alrosa_Обзор_рынка_инвестиционных_бриллиантов_октябрь_2024.pdf\n",
      "Копирован файл: Росконгресс_Рынок_промышленных_роботов_в_мире_и_России_2024_16_стр.pdf\n",
      "Копирован файл: nn_climate_change_report_rus.pdf\n",
      "Копирован файл: Норникель про корп культуру.pdf\n",
      "Копирован файл: ММК 2024.pdf\n",
      "Копирован файл: Доклад, уголь часть 1.pdf\n",
      "Копирован файл: Godovoi_-otchet-PAO-GMK-Norilskii_-nikel-za-2023-god.pdf\n",
      "Копирован файл: sr_ru_annual_report_pages_nornik_2022.pdf\n",
      "Копирован файл: Норникель_Внутрення_цена_на_углерод.pdf\n",
      "Копирован файл: 2_5282802846297776741.pdf\n",
      "Копирован и переведён в формат .pdf файл: СП_496_1325800_2020_Основания_и_фундаменты_зданий_и_сооружений.pdf\n",
      "Копирован файл: digital_production_5.pdf\n",
      "Копирован файл: NN_AR_2021_Book_RUS_26.09.22.pdf\n",
      "Копирован файл: McKinsey_Next Big Arenas_2024 (213 pgs).pdf\n",
      "Копирован файл: KPMG_Global Metals and Mining_2024 (48 pgs).pdf\n",
      "Копирован файл: NN_CSO2021_RUS_03.03.2023.pdf\n",
      "Копирован файл: 2022_Annual_Report_of_PJSC_MMC_Norilsk_Nickel_rus.pdf\n",
      "Копирован файл: НЛМК 2024.pdf\n",
      "Копирован файл: responsible_supply_chain_report_rus.pdf\n",
      "Копирован файл: Kept_Золотодобывающие_компании_2024_25_стр.pdf\n",
      "Копирован файл: ТеДо_и_СУЭК_Перспективы_развития_экспорта_российского_угля_202.pdf\n",
      "Копирован файл: nn_cso_2023_rus.pdf\n",
      "Копирован файл: 2_5366183129474161642.pdf\n",
      "Копирован файл: K2Tex_x_TeДо_Российский_рынок_ИТ_2024_30_стр.pdf\n",
      "Копирован файл: Цифровизация_горно_металлургической_отрасли_России_в_2024_г_.pdf\n",
      "Копирован и переведён в формат .pdf файл: Постановление_Правительства_РФ_от_16_02_2008_N_87_О_составе_разделов.pdf\n",
      "Копирован файл: Kept_Обзор_цен_в_металлургической_отрасли_2К2024.pdf\n"
     ]
    }
   ],
   "source": [
    "all_documents = os.listdir(input_folder)\n",
    "\n",
    "for file in all_documents:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    if file_path.endswith('.docx') or file_path.endswith('.doc'):\n",
    "        file_path = convert_doc_to_pdf(file_path, working_folder + file[:-4] + 'pdf')\n",
    "        print(f\"Копирован и переведён в формат .pdf файл: {file[:-4] + 'pdf'}\")\n",
    "    else:\n",
    "        shutil.copy(file_path, working_folder)\n",
    "        print(f\"Копирован файл: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим нашу готовую базу по индексу, который мы задали в первой части ноутбука, переведём её в работу на `cuda:0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:18:42.374703Z",
     "iopub.status.busy": "2024-12-07T08:18:42.374420Z",
     "iopub.status.idle": "2024-12-07T08:21:15.290635Z",
     "shell.execute_reply": "2024-12-07T08:21:15.289601Z",
     "shell.execute_reply.started": "2024-12-07T08:18:42.374676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390ad23964e143c0a1e4e1139ac17a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/750 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f75a105fe04ad0a301298684855ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1fd727a1d64dc3b0a7fcc5b042aad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10174abe6bbd4258ac821ffcb88180ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bc57c4b67741e0ac32629d3b8bf542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24157e4218a04b9a819e662c9d3280b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/862M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37c0d9722384bb398068bf14ec9e3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1afafa077e4647bf639bfca0672e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/78.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e0f5867f93440db9fdbb2a47a60a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b54a62eb164d4f83273c55dbbc6f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96b1c74f8c9495e83d211ba67c3d36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b756bb624fa4bb5a56ab38a27479753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/byaldi/colpali.py:162: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.indexed_embeddings.extend(torch.load(file))\n"
     ]
    }
   ],
   "source": [
    "RAG = RAGMultiModalModel.from_index(\n",
    "    index_path='nornikel_index/',\n",
    "    index_root='/kaggle/working/nornikel-2024-rag',\n",
    "    device='cuda:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По текстовому запросу определим k релевантных документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:24:52.051780Z",
     "iopub.status.busy": "2024-12-07T09:24:52.051105Z",
     "iopub.status.idle": "2024-12-07T09:24:52.865737Z",
     "shell.execute_reply": "2024-12-07T09:24:52.864820Z",
     "shell.execute_reply.started": "2024-12-07T09:24:52.051742Z"
    }
   },
   "outputs": [],
   "source": [
    "text_query = \"Расскажи про корпоративную культуру в компании Норникель\"\n",
    "results = RAG.search(text_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:24:53.851612Z",
     "iopub.status.busy": "2024-12-07T09:24:53.851176Z",
     "iopub.status.idle": "2024-12-07T09:24:53.857943Z",
     "shell.execute_reply": "2024-12-07T09:24:53.856846Z",
     "shell.execute_reply.started": "2024-12-07T09:24:53.851536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документ ID: 26, Страница: 4, Релевантность: 18.125\n",
      "Документ ID: 26, Страница: 6, Релевантность: 17.75\n",
      "Документ ID: 26, Страница: 8, Релевантность: 17.75\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    doc_id = result['doc_id']\n",
    "    page_num = result['page_num']\n",
    "    score = result['score']\n",
    "\n",
    "    print(f\"Документ ID: {doc_id}, Страница: {page_num}, Релевантность: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen-2-VL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как ColPali не считывает напрямую текст с документов и изображений, нам потребуется языковая LLM или VLM. Для ответа по релевантным страницам документов и текстовому запросу используем VLM-модель `Qwen-2-VL-2B`, а именно дообученную на русских текстах версию от Vikhrmodels: `Vikhr-2-VL-2b-Instruct-experimental`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:21:16.689746Z",
     "iopub.status.busy": "2024-12-07T08:21:16.689279Z",
     "iopub.status.idle": "2024-12-07T08:23:09.361149Z",
     "shell.execute_reply": "2024-12-07T08:23:09.360207Z",
     "shell.execute_reply.started": "2024-12-07T08:21:16.689697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548ad3ad286144cc92a2e0afe04a6519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9640a5667cb4d4d8e37ec64606ef273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdd70051f154910aaf86b82ea6ad85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1a7ed8426d404d9a9325f63cc7d2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba86681fb06a4b709163f3773c29eb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a12e224302c443e8e70b9db5cde70c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac78c3ae6074789b5d3d89839d3f4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be791667ef24e9aa2ecff9f5d226059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8178e2c424e44ea93961b7ffb554416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dcab5c812d49b7af3a8eed9c4b2668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d062ec195bc344e38a15cf75addeab1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_id = \"Vikhrmodels/Vikhr-2-VL-2b-Instruct-experimental\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"cuda:1\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мы берём релевантные страницы разных документов и переводим их в изображения. С целью оптимизации потребрения оперативной памяти снизим размер изображения примерно до 800 пикселей в длине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T08:32:38.083517Z",
     "iopub.status.busy": "2024-12-07T08:32:38.082701Z",
     "iopub.status.idle": "2024-12-07T08:32:38.088570Z",
     "shell.execute_reply": "2024-12-07T08:32:38.087653Z",
     "shell.execute_reply.started": "2024-12-07T08:32:38.083476Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_text(img, max_size=800):\n",
    "    width, height = img.size\n",
    "    if max(width, height) > max_size:\n",
    "        if width > height:\n",
    "            new_width = max_size\n",
    "            new_height = int(height * (max_size / width))\n",
    "        else:\n",
    "            new_height = max_size\n",
    "            new_width = int(width * (max_size / height))\n",
    "            \n",
    "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "        img = img.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:25:01.525774Z",
     "iopub.status.busy": "2024-12-07T09:25:01.524842Z",
     "iopub.status.idle": "2024-12-07T09:25:02.241936Z",
     "shell.execute_reply": "2024-12-07T09:25:02.240849Z",
     "shell.execute_reply.started": "2024-12-07T09:25:01.525731Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_relevant_images_ready(docs_names, results):\n",
    "    images = [docs_names[results[i]['doc_id']] for i in range(len(results))]\n",
    "    image_path = '/kaggle/working/nornikel-2024/relevant_images/'\n",
    "    \n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "    pages_to_convert = dict()\n",
    "    \n",
    "    for doc, page in zip(images, [doc['page_num'] for doc in results]):\n",
    "        pages_to_convert.setdefault(doc, []).append(page)\n",
    "    \n",
    "    ready_images_for_context = []\n",
    "    \n",
    "    for i, (pdf, pages) in enumerate(pages_to_convert.items()):\n",
    "        for j, page in enumerate(pages):\n",
    "            output_img_name = f'{image_path}/image_{i + 1}_{j + 1}.jpg'\n",
    "            \n",
    "            image = convert_from_path(pdf, first_page=page, last_page=page)\n",
    "            image = resize_text(*image)\n",
    "            image.save(output_img_name, 'JPEG')\n",
    "\n",
    "            ready_images_for_context.append(output_img_name)\n",
    "    \n",
    "\n",
    "    return ready_images_for_context\n",
    "\n",
    "relevant_images = get_relevant_images_ready(docs_names, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем контент для контектного окна, куда мы передаём наш текстовый запрос и пути по готовых изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:25:04.824766Z",
     "iopub.status.busy": "2024-12-07T09:25:04.824026Z",
     "iopub.status.idle": "2024-12-07T09:25:04.829219Z",
     "shell.execute_reply": "2024-12-07T09:25:04.828048Z",
     "shell.execute_reply.started": "2024-12-07T09:25:04.824725Z"
    }
   },
   "outputs": [],
   "source": [
    "content = [\n",
    "    *[{\"type\": \"image\", \"image\": image} for image in relevant_images],\n",
    "    {\"type\": \"text\", \"text\": text_query}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:25:06.511043Z",
     "iopub.status.busy": "2024-12-07T09:25:06.510658Z",
     "iopub.status.idle": "2024-12-07T09:25:06.518177Z",
     "shell.execute_reply": "2024-12-07T09:25:06.517142Z",
     "shell.execute_reply.started": "2024-12-07T09:25:06.511011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Ты бизнес-ассистент. Отвечай на вопрос точно.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': '/kaggle/working/nornikel-2024/relevant_images//image_1_1.jpg'},\n",
       "   {'type': 'image',\n",
       "    'image': '/kaggle/working/nornikel-2024/relevant_images//image_1_2.jpg'},\n",
       "   {'type': 'image',\n",
       "    'image': '/kaggle/working/nornikel-2024/relevant_images//image_1_3.jpg'},\n",
       "   {'type': 'text',\n",
       "    'text': 'Расскажи про корпоративную культуру в компании Норникель'}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Ты бизнес-ассистент. Отвечай на вопрос точно.\"}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": content,\n",
    "    }\n",
    "]\n",
    "\n",
    "display(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:25:09.560806Z",
     "iopub.status.busy": "2024-12-07T09:25:09.560377Z",
     "iopub.status.idle": "2024-12-07T09:25:09.680209Z",
     "shell.execute_reply": "2024-12-07T09:25:09.679128Z",
     "shell.execute_reply.started": "2024-12-07T09:25:09.560770Z"
    }
   },
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После передачи текстового запроса в модель получаем готовый сгенерированный моделью ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T09:25:10.722474Z",
     "iopub.status.busy": "2024-12-07T09:25:10.721547Z",
     "iopub.status.idle": "2024-12-07T09:25:16.768994Z",
     "shell.execute_reply": "2024-12-07T09:25:16.767888Z",
     "shell.execute_reply.started": "2024-12-07T09:25:10.722437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мы говорим о предпочтениях и изменениях в бизнесе, которые вносят вклад в культуру компании, а также о роли руководства в процессе формирования и оптимизации культуры компании. Критерии оценки включают: уровень готовности, уровень сотрудничества, уровень развития, уровень обучения, уровень коммуникации и уровень успеха.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=2048,\n",
    "    temperature=0.3,\n",
    "    top_k=100,\n",
    "    top_p=0.95\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6245083,
     "sourceId": 10120929,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6247002,
     "sourceId": 10123533,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
